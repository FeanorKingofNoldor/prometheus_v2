"""Prometheus v2 – Market-aware DAG orchestration daemon.

This module implements the production market-aware orchestrator that combines:
- Real-time market state detection (trading hours, holidays)
- DAG-based dependency resolution
- Job execution with retry logic and timeout monitoring
- Persistent state tracking in job_executions table

The daemon monitors multiple markets in a follow-the-sun pattern, executing
jobs when:
1. The market is in the required state (e.g., POST_CLOSE for ingestion)
2. All job dependencies have been satisfied
3. Previous attempts have not exceeded retry limits

Design goals:
- **Idempotent**: Jobs can be safely re-run
- **Resilient**: Graceful handling of failures with exponential backoff
- **Observable**: All executions tracked in database for monitoring
- **Non-blocking**: Per-market DAGs execute independently
"""

from __future__ import annotations

import argparse
import random
import signal
import sys
import time
import traceback
from dataclasses import dataclass
from datetime import date, datetime, timedelta, timezone
from typing import Dict, List, Optional, Set, Tuple

from prometheus.core.database import DatabaseManager, get_db_manager
from prometheus.core.ids import generate_uuid
from prometheus.core.logging import get_logger
from prometheus.core.market_state import MarketState, get_market_state, get_next_state_transition
from prometheus.orchestration.dag import (
    DAG,
    JobMetadata,
    JobStatus,
    build_market_dag,
)
from prometheus.pipeline.tasks import (
    run_signals_for_run,
    run_universes_for_run,
    run_books_for_run,
)
from prometheus.pipeline.state import EngineRun, RunPhase, update_phase
from prometheus.data_ingestion.daily_orchestrator import run_daily_ingestion, is_data_ready_for_market

logger = get_logger(__name__)


# ============================================================================
# Job Execution Tracking
# ============================================================================


@dataclass
class JobExecution:
    """Represents a job execution record from the database."""

    execution_id: str
    job_id: str
    job_type: str
    dag_id: str
    market_id: str | None
    as_of_date: date
    status: JobStatus
    started_at: datetime | None
    completed_at: datetime | None
    attempt_number: int
    error_message: str | None
    error_details: dict | None
    created_at: datetime
    updated_at: datetime


def create_job_execution(
    db_manager: DatabaseManager,
    job: JobMetadata,
    dag_id: str,
    as_of_date: date,
) -> JobExecution:
    """Create a new PENDING job execution record."""
    execution_id = generate_uuid()
    now = datetime.now(timezone.utc).replace(tzinfo=None)

    sql = """
        INSERT INTO job_executions (
            execution_id, job_id, job_type, dag_id, market_id, as_of_date,
            status, attempt_number, created_at, updated_at
        )
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
    """

    with db_manager.get_runtime_connection() as conn:
        cursor = conn.cursor()
        try:
            cursor.execute(
                sql,
                (
                    execution_id,
                    job.job_id,
                    job.job_type,
                    dag_id,
                    job.market_id,
                    as_of_date,
                    JobStatus.PENDING.value,
                    1,
                    now,
                    now,
                ),
            )
            conn.commit()
        finally:
            cursor.close()

    return JobExecution(
        execution_id=execution_id,
        job_id=job.job_id,
        job_type=job.job_type,
        dag_id=dag_id,
        market_id=job.market_id,
        as_of_date=as_of_date,
        status=JobStatus.PENDING,
        started_at=None,
        completed_at=None,
        attempt_number=1,
        error_message=None,
        error_details=None,
        created_at=now,
        updated_at=now,
    )


def update_job_execution_status(
    db_manager: DatabaseManager,
    execution_id: str,
    status: JobStatus,
    error_message: str | None = None,
    error_details: dict | None = None,
) -> None:
    """Update the status of a job execution."""
    now = datetime.now(timezone.utc).replace(tzinfo=None)

    # Set started_at when transitioning to RUNNING
    # Set completed_at when transitioning to terminal states
    if status == JobStatus.RUNNING:
        sql = """
            UPDATE job_executions
            SET status = %s, started_at = %s, updated_at = %s
            WHERE execution_id = %s
        """
        params = (status.value, now, now, execution_id)
    elif status in {JobStatus.SUCCESS, JobStatus.FAILED, JobStatus.SKIPPED}:
        sql = """
            UPDATE job_executions
            SET status = %s, completed_at = %s, updated_at = %s,
                error_message = %s, error_details = %s
            WHERE execution_id = %s
        """
        import json

        params = (
            status.value,
            now,
            now,
            error_message,
            json.dumps(error_details) if error_details else None,
            execution_id,
        )
    else:
        sql = """
            UPDATE job_executions
            SET status = %s, updated_at = %s,
                error_message = %s, error_details = %s
            WHERE execution_id = %s
        """
        import json

        params = (
            status.value,
            now,
            error_message,
            json.dumps(error_details) if error_details else None,
            execution_id,
        )

    with db_manager.get_runtime_connection() as conn:
        cursor = conn.cursor()
        try:
            cursor.execute(sql, params)
            conn.commit()
        finally:
            cursor.close()


def get_dag_executions(
    db_manager: DatabaseManager,
    dag_id: str,
) -> List[JobExecution]:
    """Load all job executions for a DAG ordered by creation time."""
    sql = """
        SELECT execution_id, job_id, job_type, dag_id, market_id, as_of_date,
               status, started_at, completed_at, attempt_number,
               error_message, error_details, created_at, updated_at
        FROM job_executions
        WHERE dag_id = %s
        ORDER BY created_at DESC
    """

    with db_manager.get_runtime_connection() as conn:
        cursor = conn.cursor()
        try:
            cursor.execute(sql, (dag_id,))
            rows = cursor.fetchall()
        finally:
            cursor.close()

    executions = []
    for row in rows:
        import json

        error_details = json.loads(row[11]) if row[11] else None
        executions.append(
            JobExecution(
                execution_id=row[0],
                job_id=row[1],
                job_type=row[2],
                dag_id=row[3],
                market_id=row[4],
                as_of_date=row[5],
                status=JobStatus(row[6]),
                started_at=row[7],
                completed_at=row[8],
                attempt_number=row[9],
                error_message=row[10],
                error_details=error_details,
                created_at=row[12],
                updated_at=row[13],
            )
        )

    return executions


def get_latest_job_execution(
    db_manager: DatabaseManager,
    job_id: str,
    dag_id: str,
) -> JobExecution | None:
    """Get the most recent execution for a specific job in a DAG."""
    sql = """
        SELECT execution_id, job_id, job_type, dag_id, market_id, as_of_date,
               status, started_at, completed_at, attempt_number,
               error_message, error_details, created_at, updated_at
        FROM job_executions
        WHERE job_id = %s AND dag_id = %s
        ORDER BY created_at DESC
        LIMIT 1
    """

    with db_manager.get_runtime_connection() as conn:
        cursor = conn.cursor()
        try:
            cursor.execute(sql, (job_id, dag_id))
            row = cursor.fetchone()
        finally:
            cursor.close()

    if not row:
        return None

    import json

    error_details = json.loads(row[11]) if row[11] else None
    return JobExecution(
        execution_id=row[0],
        job_id=row[1],
        job_type=row[2],
        dag_id=row[3],
        market_id=row[4],
        as_of_date=row[5],
        status=JobStatus(row[6]),
        started_at=row[7],
        completed_at=row[8],
        attempt_number=row[9],
        error_message=row[10],
        error_details=error_details,
        created_at=row[12],
        updated_at=row[13],
    )


def increment_job_execution_attempt(
    db_manager: DatabaseManager,
    execution_id: str,
) -> None:
    """Increment the attempt number for a job execution (for retries)."""
    sql = """
        UPDATE job_executions
        SET attempt_number = attempt_number + 1,
            status = %s,
            updated_at = %s
        WHERE execution_id = %s
    """
    now = datetime.now(timezone.utc).replace(tzinfo=None)

    with db_manager.get_runtime_connection() as conn:
        cursor = conn.cursor()
        try:
            cursor.execute(sql, (JobStatus.PENDING.value, now, execution_id))
            conn.commit()
        finally:
            cursor.close()


# ============================================================================
# Job Execution Logic
# ============================================================================


def _get_or_create_engine_run(
    db_manager: DatabaseManager,
    market_id: str,
    as_of_date: date,
) -> EngineRun | None:
    """Get or create an EngineRun for the given market and date.

    Returns None if the region cannot be inferred from market_id.
    """
    # Map market_id to region (inverse of MARKETS_BY_REGION in tasks.py)
    REGION_MAP = {
        "US_EQ": "US",
        "EU_EQ": "EU",
        "ASIA_EQ": "ASIA",
    }

    region = REGION_MAP.get(market_id)
    if not region:
        logger.warning(
            "_get_or_create_engine_run: unknown market_id=%s, cannot create EngineRun", market_id
        )
        return None

    # Check if run exists
    sql = """
        SELECT run_id, region, as_of_date, phase, created_at, updated_at
        FROM engine_runs
        WHERE region = %s AND as_of_date = %s
        LIMIT 1
    """

    with db_manager.get_runtime_connection() as conn:
        cursor = conn.cursor()
        try:
            cursor.execute(sql, (region, as_of_date))
            row = cursor.fetchone()
        finally:
            cursor.close()

    if row:
        return EngineRun(
            run_id=row[0],
            region=row[1],
            as_of_date=row[2],
            phase=RunPhase(row[3]),
            created_at=row[4],
            updated_at=row[5],
        )

    # Create new run in DATA_READY state (orchestrator manages data readiness)
    run_id = generate_uuid()
    now = datetime.now(timezone.utc).replace(tzinfo=None)

    sql = """
        INSERT INTO engine_runs (run_id, region, as_of_date, phase, created_at, updated_at)
        VALUES (%s, %s, %s, %s, %s, %s)
    """

    with db_manager.get_runtime_connection() as conn:
        cursor = conn.cursor()
        try:
            cursor.execute(sql, (run_id, region, as_of_date, RunPhase.DATA_READY.value, now, now))
            conn.commit()
        finally:
            cursor.close()

    logger.info(
        "_get_or_create_engine_run: created run_id=%s region=%s as_of_date=%s",
        run_id,
        region,
        as_of_date,
    )

    return EngineRun(
        run_id=run_id,
        region=region,
        as_of_date=as_of_date,
        phase=RunPhase.DATA_READY,
        created_at=now,
        updated_at=now,
    )


def execute_job(
    db_manager: DatabaseManager,
    job: JobMetadata,
    execution: JobExecution,
) -> Tuple[bool, str | None]:
    """Execute a single job.

    Returns:
        (success: bool, error_message: str | None)
    """
    logger.info(
        "execute_job: job_type=%s job_id=%s execution_id=%s attempt=%d",
        job.job_type,
        job.job_id,
        execution.execution_id,
        execution.attempt_number,
    )

    try:
        # Get or create EngineRun
        run = _get_or_create_engine_run(db_manager, job.market_id, execution.as_of_date)
        if not run:
            return False, f"Could not create EngineRun for market_id={job.market_id}"

        # Execute based on job type
        if job.job_type == "ingest_prices":
            # Run complete daily ingestion workflow
            result = run_daily_ingestion(
                db_manager,
                job.market_id,
                execution.as_of_date,
            )
            
            if result.status.value == "COMPLETE":
                # Check if data is ready for processing
                if is_data_ready_for_market(db_manager, job.market_id, execution.as_of_date):
                    # Mark engine run as DATA_READY
                    if run.phase == RunPhase.WAITING_FOR_DATA:
                        update_phase(db_manager, run.run_id, RunPhase.DATA_READY)
                return True, None
            else:
                return False, result.error_message

        elif job.job_type == "ingest_factors":
            # Similar to ingest_prices
            if run.phase == RunPhase.WAITING_FOR_DATA:
                update_phase(db_manager, run.run_id, RunPhase.DATA_READY)
            return True, None

        elif job.job_type == "compute_returns":
            # Returns are computed during backfill or on-demand
            # Mark as success if we're past DATA_READY
            return run.phase != RunPhase.WAITING_FOR_DATA, None

        elif job.job_type == "compute_volatility":
            # Volatility computed during backfill
            return run.phase != RunPhase.WAITING_FOR_DATA, None

        elif job.job_type == "build_numeric_windows":
            # Numeric embeddings backfilled separately
            return run.phase != RunPhase.WAITING_FOR_DATA, None

        elif job.job_type == "update_profiles":
            # Profiles are updated as part of run_signals_for_run
            # This is a no-op marker for dependency ordering
            return run.phase != RunPhase.WAITING_FOR_DATA, None

        elif job.job_type == "run_signals":
            # Execute signals phase
            if run.phase == RunPhase.DATA_READY:
                run_signals_for_run(db_manager, run)
            return True, None

        elif job.job_type == "run_universes":
            # Execute universes phase
            if run.phase == RunPhase.SIGNALS_DONE:
                run_universes_for_run(db_manager, run)
            return True, None

        elif job.job_type == "run_books":
            # Execute books phase
            if run.phase == RunPhase.UNIVERSES_DONE:
                run_books_for_run(db_manager, run)
            return True, None

        else:
            return False, f"Unknown job_type: {job.job_type}"

    except Exception as exc:
        error_msg = f"{type(exc).__name__}: {exc}"
        logger.exception("execute_job: failed job_id=%s: %s", job.job_id, error_msg)
        return False, error_msg


# ============================================================================
# Retry Logic
# ============================================================================


def calculate_retry_delay(
    job: JobMetadata,
    attempt_number: int,
) -> float:
    """Calculate exponential backoff delay with jitter.

    Returns delay in seconds.
    """
    base_delay = job.retry_delay_seconds
    # Exponential backoff: base * 2^(attempt - 1)
    delay = base_delay * (2 ** (attempt_number - 1))
    # Add jitter: ±25%
    jitter = delay * 0.25 * (2 * random.random() - 1)
    return max(1.0, delay + jitter)


def should_retry_job(
    job: JobMetadata,
    execution: JobExecution,
) -> bool:
    """Determine if a failed job should be retried."""
    if execution.status != JobStatus.FAILED:
        return False

    if execution.attempt_number >= job.max_retries:
        logger.info(
            "should_retry_job: job_id=%s exhausted retries (%d/%d)",
            job.job_id,
            execution.attempt_number,
            job.max_retries,
        )
        return False

    return True


# ============================================================================
# Market-Aware Daemon
# ============================================================================


@dataclass(frozen=True)
class MarketAwareDaemonConfig:
    """Configuration for the market-aware orchestrator daemon.

    Attributes:
        markets: List of market IDs to orchestrate (e.g., ["US_EQ", "EU_EQ"])
        poll_interval_seconds: Sleep interval between polling cycles
        as_of_date: Optional fixed date for orchestration (defaults to today)
    """

    markets: List[str]
    poll_interval_seconds: int = 60
    as_of_date: date | None = None


class MarketAwareDaemon:
    """Market-aware DAG orchestration daemon.

    Manages execution of market-specific DAGs based on real-time trading
    hours and dependency resolution.
    """

    def __init__(
        self,
        config: MarketAwareDaemonConfig,
        db_manager: DatabaseManager,
    ):
        self.config = config
        self.db_manager = db_manager
        self.shutdown_requested = False

        # Track active DAGs: {market_id: (DAG, dag_id)}
        self.active_dags: Dict[str, Tuple[DAG, str]] = {}

        # Track running jobs: {execution_id: (job, start_time)}
        self.running_jobs: Dict[str, Tuple[JobMetadata, datetime]] = {}

        # Track retry backoff: {execution_id: retry_after_timestamp}
        self.retry_backoff: Dict[str, datetime] = {}

    def _setup_signal_handlers(self) -> None:
        """Setup graceful shutdown handlers."""

        def _signal_handler(signum, frame):
            logger.info("MarketAwareDaemon: received signal %d, shutting down", signum)
            self.shutdown_requested = True

        signal.signal(signal.SIGTERM, _signal_handler)
        signal.signal(signal.SIGINT, _signal_handler)

    def _initialize_dags(self, as_of_date: date) -> None:
        """Initialize or refresh DAGs for all configured markets."""
        for market_id in self.config.markets:
            dag = build_market_dag(market_id, as_of_date)
            dag_id = f"{market_id}_{as_of_date.isoformat()}"
            self.active_dags[market_id] = (dag, dag_id)
            logger.info(
                "_initialize_dags: initialized dag_id=%s with %d jobs",
                dag_id,
                len(dag.jobs),
            )

    def _get_completed_jobs(self, dag_id: str) -> Set[str]:
        """Get set of successfully completed job IDs for a DAG."""
        executions = get_dag_executions(self.db_manager, dag_id)
        return {
            exec.job_id
            for exec in executions
            if exec.status == JobStatus.SUCCESS
        }

    def _get_running_job_ids(self) -> Set[str]:
        """Get set of currently running job IDs."""
        return {job.job_id for job, _ in self.running_jobs.values()}

    def _check_timeouts(self, now: datetime) -> None:
        """Check for timed-out jobs and mark them as failed."""
        timed_out = []

        for execution_id, (job, start_time) in self.running_jobs.items():
            elapsed = (now - start_time).total_seconds()
            if elapsed > job.timeout_seconds:
                timed_out.append(execution_id)
                logger.warning(
                    "_check_timeouts: job_id=%s timed out after %.1fs (limit: %ds)",
                    job.job_id,
                    elapsed,
                    job.timeout_seconds,
                )

        for execution_id in timed_out:
            update_job_execution_status(
                self.db_manager,
                execution_id,
                JobStatus.FAILED,
                error_message=f"Job timed out after {job.timeout_seconds}s",
            )
            del self.running_jobs[execution_id]

    def _process_market(
        self,
        market_id: str,
        dag: DAG,
        dag_id: str,
        current_state: MarketState,
        as_of_date: date,
        now: datetime,
    ) -> None:
        """Process one market's DAG for the current cycle."""
        # Get DAG state
        completed = self._get_completed_jobs(dag_id)
        running = self._get_running_job_ids()

        # Get runnable jobs
        runnable = dag.get_runnable_jobs(completed, running, current_state)

        if not runnable:
            return

        logger.info(
            "_process_market: market_id=%s state=%s runnable=%d completed=%d running=%d",
            market_id,
            current_state.value,
            len(runnable),
            len(completed),
            len(running),
        )

        # Execute runnable jobs
        for job in runnable:
            # Check if we're in retry backoff
            latest_exec = get_latest_job_execution(self.db_manager, job.job_id, dag_id)

            if latest_exec and latest_exec.execution_id in self.retry_backoff:
                retry_after = self.retry_backoff[latest_exec.execution_id]
                if now < retry_after:
                    logger.debug(
                        "_process_market: job_id=%s in backoff until %s",
                        job.job_id,
                        retry_after,
                    )
                    continue
                else:
                    # Backoff expired, remove from tracking
                    del self.retry_backoff[latest_exec.execution_id]

            # Create or reuse execution record
            if latest_exec and latest_exec.status == JobStatus.PENDING:
                execution = latest_exec
            elif latest_exec and should_retry_job(job, latest_exec):
                # Increment attempt and retry
                increment_job_execution_attempt(self.db_manager, latest_exec.execution_id)
                execution = get_latest_job_execution(self.db_manager, job.job_id, dag_id)
            else:
                # Create new execution
                execution = create_job_execution(self.db_manager, job, dag_id, as_of_date)

            # Mark as running
            update_job_execution_status(self.db_manager, execution.execution_id, JobStatus.RUNNING)
            self.running_jobs[execution.execution_id] = (job, now)

            # Execute job
            success, error_msg = execute_job(self.db_manager, job, execution)

            # Update status
            if success:
                update_job_execution_status(
                    self.db_manager,
                    execution.execution_id,
                    JobStatus.SUCCESS,
                )
                logger.info(
                    "_process_market: job_id=%s SUCCESS (execution_id=%s)",
                    job.job_id,
                    execution.execution_id,
                )
            else:
                update_job_execution_status(
                    self.db_manager,
                    execution.execution_id,
                    JobStatus.FAILED,
                    error_message=error_msg,
                )
                logger.error(
                    "_process_market: job_id=%s FAILED (execution_id=%s): %s",
                    job.job_id,
                    execution.execution_id,
                    error_msg,
                )

                # Schedule retry if applicable
                if should_retry_job(job, execution):
                    delay = calculate_retry_delay(job, execution.attempt_number)
                    retry_after = now + timedelta(seconds=delay)
                    self.retry_backoff[execution.execution_id] = retry_after
                    logger.info(
                        "_process_market: job_id=%s will retry in %.1fs (attempt %d/%d)",
                        job.job_id,
                        delay,
                        execution.attempt_number + 1,
                        job.max_retries,
                    )

            # Remove from running
            if execution.execution_id in self.running_jobs:
                del self.running_jobs[execution.execution_id]

    def _run_cycle(self, as_of_date: date) -> None:
        """Execute one orchestration cycle across all markets."""
        now = datetime.now(timezone.utc)

        # Check for timeouts
        self._check_timeouts(now)

        # Process each market
        for market_id in self.config.markets:
            if market_id not in self.active_dags:
                continue

            dag, dag_id = self.active_dags[market_id]
            current_state = get_market_state(market_id, now)

            self._process_market(market_id, dag, dag_id, current_state, as_of_date, now)

    def run(self) -> None:
        """Run the orchestration daemon until shutdown is requested."""
        self._setup_signal_handlers()

        as_of_date = self.config.as_of_date or date.today()
        self._initialize_dags(as_of_date)

        logger.info(
            "MarketAwareDaemon: starting markets=%s as_of_date=%s poll_interval=%ds",
            ",".join(self.config.markets),
            as_of_date,
            self.config.poll_interval_seconds,
        )

        cycle_count = 0
        while not self.shutdown_requested:
            try:
                cycle_count += 1
                logger.debug("MarketAwareDaemon: cycle %d starting", cycle_count)

                self._run_cycle(as_of_date)

                # Sleep until next cycle
                time.sleep(self.config.poll_interval_seconds)

            except Exception as exc:  # pragma: no cover - defensive
                logger.exception("MarketAwareDaemon: cycle %d failed: %s", cycle_count, exc)
                time.sleep(self.config.poll_interval_seconds)

        logger.info("MarketAwareDaemon: shutdown complete after %d cycles", cycle_count)


# ============================================================================
# CLI Entrypoint
# ============================================================================


def _parse_args(argv: Optional[List[str]]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Prometheus v2 market-aware DAG orchestration daemon"
    )

    parser.add_argument(
        "--market",
        action="append",
        required=True,
        help="Market ID to orchestrate (e.g., US_EQ). Can specify multiple times.",
    )
    parser.add_argument(
        "--poll-interval-seconds",
        type=int,
        default=60,
        help="Sleep interval between polling cycles (default: 60)",
    )
    parser.add_argument(
        "--as-of-date",
        type=str,
        help="Fixed as-of date for orchestration (YYYY-MM-DD). Defaults to today.",
    )

    args = parser.parse_args(argv)

    if args.poll_interval_seconds <= 0:
        parser.error("--poll-interval-seconds must be positive")

    if args.as_of_date:
        try:
            args.as_of_date = datetime.strptime(args.as_of_date, "%Y-%m-%d").date()
        except ValueError:
            parser.error("--as-of-date must be in YYYY-MM-DD format")

    return args


def main(argv: Optional[List[str]] = None) -> None:
    """CLI entrypoint for the market-aware daemon.

    Example::

        python -m prometheus.orchestration.market_aware_daemon \\
            --market US_EQ \\
            --market EU_EQ \\
            --poll-interval-seconds 60
    """
    args = _parse_args(argv)

    config = MarketAwareDaemonConfig(
        markets=args.market,
        poll_interval_seconds=args.poll_interval_seconds,
        as_of_date=args.as_of_date,
    )

    db_manager = get_db_manager()
    daemon = MarketAwareDaemon(config, db_manager)
    daemon.run()


if __name__ == "__main__":  # pragma: no cover
    main()
